{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 1387322\n"
     ]
    }
   ],
   "source": [
    "# Load the pickled data\n",
    "import pickle\n",
    "\n",
    "file_location = 'D:\\\\5005-Data\\\\tweet_combined_with_sentiment.pkl'\n",
    "\n",
    "with open(file_location, 'rb') as f:\n",
    "    tweets_dict = pickle.load(f)\n",
    "\n",
    "print('Total number of tweets:', len(tweets_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets that are not reply: 776029\n",
      "Top 50 perecent like ratio: 0.0\n",
      "Top 90 perecent like ratio: 0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "like_ratios = []\n",
    "for k, tweet in tweets_dict.items():\n",
    "    if tweet.viewCount is None:\n",
    "        continue\n",
    "    if tweet.lang != 'en':\n",
    "        continue\n",
    "    if tweet.inReplyToUser is not None:\n",
    "        continue\n",
    "    ratio = tweet.likeCount / tweet.viewCount\n",
    "    like_ratios.append(ratio)\n",
    "\n",
    "p50 = np.percentile(like_ratios, 50)\n",
    "p90 = np.percentile(like_ratios, 90)\n",
    "\n",
    "print('Number of tweets that are not reply:', len(like_ratios))\n",
    "print('Top 50 perecent like ratio:', p50)\n",
    "print('Top 90 perecent like ratio:', p90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls_and_entities(text):\n",
    "    \"\"\"\n",
    "    Removes URLs and HTML entities from a string using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input string to remove URLs and HTML entities from.\n",
    "        \n",
    "    Returns:\n",
    "        str: The input string with any URLs and HTML entities removed.\n",
    "    \"\"\"\n",
    "    # Define regular expressions to match URLs and HTML entities\n",
    "    url_pattern = re.compile(r'https?://(?:www\\.\\S+|(?!www)\\S+)')\n",
    "    entity_pattern = re.compile(r'&\\w+;')\n",
    "    \n",
    "    # Use the sub() method to replace URLs and HTML entities with an empty string\n",
    "    text_without_urls_and_entities = url_pattern.sub('', text)\n",
    "    text_without_urls_and_entities = entity_pattern.sub('', text_without_urls_and_entities)\n",
    "    text_without_urls_and_entities = text_without_urls_and_entities.replace('\\n', ' ')\n",
    "    \n",
    "    return text_without_urls_and_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for k, tweet in tweets_dict.items():\n",
    "    if tweet.viewCount is None:\n",
    "        continue\n",
    "    if tweet.lang != 'en':\n",
    "        continue\n",
    "    if tweet.inReplyToUser is not None:\n",
    "        continue\n",
    "    content = remove_urls_and_entities(tweet.rawContent)\n",
    "    label = 0\n",
    "    if tweet.likeCount / tweet.viewCount > 0:\n",
    "        label = 1\n",
    "    if tweet.likeCount / tweet.viewCount > 0.025:\n",
    "        label = 2\n",
    "    lines.append('__label__' + str(label) + ' ' + content)\n",
    "\n",
    "part = int(len(lines) * 0.1)\n",
    "\n",
    "f = open('train.ftxt', 'w', encoding='utf-8')\n",
    "train_lines = lines[:part * 8]\n",
    "f.write('\\n'.join(train_lines))\n",
    "f.close()\n",
    "\n",
    "f = open('test.ftxt', 'w', encoding='utf-8')\n",
    "test_lines = lines[part * 8:part * 9]\n",
    "f.write('\\n'.join(test_lines))\n",
    "f.close()\n",
    "\n",
    "f = open('dev.ftxt', 'w', encoding='utf-8')\n",
    "test_lines = lines[part * 9:]\n",
    "f.write('\\n'.join(test_lines))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-16 20:27:45,734 Reading data from .\n",
      "2023-04-16 20:27:45,734 Train: train.ftxt\n",
      "2023-04-16 20:27:45,734 Dev: dev.ftxt\n",
      "2023-04-16 20:27:45,735 Test: test.ftxt\n",
      "2023-04-16 20:28:05,417 Initialized corpus . (label type name is 'class')\n",
      "2023-04-16 20:28:08,781 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27704\\3049511425.py:9: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
      "620816it [02:14, 4616.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-16 20:30:23,254 Dictionary created for label 'class' with 4 values: 0 (seen 308976 times), 1 (seen 248183 times), 2 (seen 63657 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "corpus = ClassificationCorpus(Path('./'), test_file='test.ftxt', dev_file='dev.ftxt', train_file='train.ftxt')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_type='class', label_dictionary=corpus.make_label_dictionary('class'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('./', max_epochs=2, learning_rate=0.05, mini_batch_size=64, monitor_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5686951591913517\n"
     ]
    }
   ],
   "source": [
    "total_test = len(test_lines)\n",
    "\n",
    "with open('test.tsv', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    wrong_num = content.count('-> MISMATCH!')\n",
    "\n",
    "print('Accuracy:', 1 - wrong_num / total_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
