{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387322"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pickled data\n",
    "import pickle\n",
    "\n",
    "file_location = 'D:\\\\5005-Data\\\\tweet_combined_with_sentiment.pkl'\n",
    "\n",
    "with open(file_location, 'rb') as f:\n",
    "    tweets_dict = pickle.load(f)\n",
    "\n",
    "len(tweets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776029\n",
      "0.0 0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "like_ratios = []\n",
    "for k, tweet in tweets_dict.items():\n",
    "    if tweet.viewCount is None:\n",
    "        continue\n",
    "    if tweet.lang != 'en':\n",
    "        continue\n",
    "    if tweet.inReplyToUser is not None:\n",
    "        continue\n",
    "    ratio = tweet.likeCount / tweet.viewCount\n",
    "    like_ratios.append(ratio)\n",
    "\n",
    "p50 = np.percentile(like_ratios, 50)\n",
    "p90 = np.percentile(like_ratios, 90)\n",
    "\n",
    "print(len(like_ratios))\n",
    "print(p50, p90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls_and_entities(text):\n",
    "    \"\"\"\n",
    "    Removes URLs and HTML entities from a string using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input string to remove URLs and HTML entities from.\n",
    "        \n",
    "    Returns:\n",
    "        str: The input string with any URLs and HTML entities removed.\n",
    "    \"\"\"\n",
    "    # Define regular expressions to match URLs and HTML entities\n",
    "    url_pattern = re.compile(r'https?://(?:www\\.\\S+|(?!www)\\S+)')\n",
    "    entity_pattern = re.compile(r'&\\w+;')\n",
    "    \n",
    "    # Use the sub() method to replace URLs and HTML entities with an empty string\n",
    "    text_without_urls_and_entities = url_pattern.sub('', text)\n",
    "    text_without_urls_and_entities = entity_pattern.sub('', text_without_urls_and_entities)\n",
    "    text_without_urls_and_entities = text_without_urls_and_entities.replace('\\n', ' ')\n",
    "    \n",
    "    return text_without_urls_and_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for k, tweet in tweets_dict.items():\n",
    "    if tweet.viewCount is None:\n",
    "        continue\n",
    "    if tweet.lang != 'en':\n",
    "        continue\n",
    "    if tweet.inReplyToUser is not None:\n",
    "        continue\n",
    "    content = remove_urls_and_entities(tweet.rawContent)\n",
    "    label = 0\n",
    "    if tweet.likeCount / tweet.viewCount > 0:\n",
    "        label = 1\n",
    "    if tweet.likeCount / tweet.viewCount > 0.025:\n",
    "        label = 2\n",
    "    lines.append('__label__' + str(label) + ' ' + content)\n",
    "\n",
    "part = int(len(lines) * 0.1)\n",
    "\n",
    "f = open('train.ftxt', 'w', encoding='utf-8')\n",
    "train_lines = lines[:part * 8]\n",
    "f.write('\\n'.join(train_lines))\n",
    "f.close()\n",
    "\n",
    "f = open('test.ftxt', 'w', encoding='utf-8')\n",
    "test_lines = lines[part * 8:part * 9]\n",
    "f.write('\\n'.join(test_lines))\n",
    "f.close()\n",
    "\n",
    "f = open('dev.ftxt', 'w', encoding='utf-8')\n",
    "test_lines = lines[part * 9:]\n",
    "f.write('\\n'.join(test_lines))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 19:05:53,680 Reading data from .\n",
      "2023-04-02 19:05:53,680 Train: train.ftxt\n",
      "2023-04-02 19:05:53,680 Dev: dev.ftxt\n",
      "2023-04-02 19:05:53,681 Test: test.ftxt\n",
      "2023-04-02 19:06:13,735 Initialized corpus . (label type name is 'class')\n",
      "2023-04-02 19:06:16,628 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9248\\3049511425.py:9: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
      "620816it [02:08, 4818.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 19:08:25,478 Dictionary created for label 'class' with 4 values: 0 (seen 308976 times), 1 (seen 248183 times), 2 (seen 63657 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "corpus = ClassificationCorpus(Path('./'), test_file='test.ftxt', dev_file='dev.ftxt', train_file='train.ftxt')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_type='class', label_dictionary=corpus.make_label_dictionary('class'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 19:30:25,757 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:30:25,757 Model: \"TextClassifier(\n",
      "  (embeddings): DocumentLSTMEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings(\n",
      "        'glove'\n",
      "        (embedding): Embedding(400001, 100)\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2023-04-02 19:30:25,758 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:30:25,759 Corpus: \"Corpus: 620816 train + 77611 dev + 77602 test sentences\"\n",
      "2023-04-02 19:30:25,759 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:30:25,759 Parameters:\n",
      "2023-04-02 19:30:25,760  - learning_rate: \"0.050000\"\n",
      "2023-04-02 19:30:25,760  - mini_batch_size: \"64\"\n",
      "2023-04-02 19:30:25,761  - patience: \"3\"\n",
      "2023-04-02 19:30:25,761  - anneal_factor: \"0.5\"\n",
      "2023-04-02 19:30:25,761  - max_epochs: \"5\"\n",
      "2023-04-02 19:30:25,762  - shuffle: \"True\"\n",
      "2023-04-02 19:30:25,762  - train_with_dev: \"False\"\n",
      "2023-04-02 19:30:25,762  - batch_growth_annealing: \"False\"\n",
      "2023-04-02 19:30:25,763 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:30:25,763 Model training base path: \".\"\n",
      "2023-04-02 19:30:25,764 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:30:25,764 Device: cuda:0\n",
      "2023-04-02 19:30:25,764 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:30:25,765 Embeddings storage mode: cpu\n",
      "2023-04-02 19:30:25,765 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 19:36:10,776 epoch 1 - iter 970/9701 - loss 0.96213266 - time (sec): 345.01 - samples/sec: 179.94 - lr: 0.050000\n",
      "2023-04-02 19:41:54,863 epoch 1 - iter 1940/9701 - loss 0.94460362 - time (sec): 689.10 - samples/sec: 180.18 - lr: 0.050000\n",
      "2023-04-02 19:47:06,646 epoch 1 - iter 2910/9701 - loss 0.93158684 - time (sec): 1000.88 - samples/sec: 186.08 - lr: 0.050000\n",
      "2023-04-02 19:52:30,680 epoch 1 - iter 3880/9701 - loss 0.92191890 - time (sec): 1324.91 - samples/sec: 187.42 - lr: 0.050000\n",
      "2023-04-02 19:58:06,076 epoch 1 - iter 4850/9701 - loss 0.91695012 - time (sec): 1660.31 - samples/sec: 186.95 - lr: 0.050000\n",
      "2023-04-02 20:03:50,218 epoch 1 - iter 5820/9701 - loss 0.91432928 - time (sec): 2004.45 - samples/sec: 185.83 - lr: 0.050000\n",
      "2023-04-02 20:09:19,877 epoch 1 - iter 6790/9701 - loss 0.91035195 - time (sec): 2334.11 - samples/sec: 186.18 - lr: 0.050000\n",
      "2023-04-02 20:14:50,249 epoch 1 - iter 7760/9701 - loss 0.91172610 - time (sec): 2664.48 - samples/sec: 186.39 - lr: 0.050000\n",
      "2023-04-02 20:20:44,966 epoch 1 - iter 8730/9701 - loss 0.91333404 - time (sec): 3019.20 - samples/sec: 185.06 - lr: 0.050000\n",
      "2023-04-02 20:26:00,951 epoch 1 - iter 9700/9701 - loss 0.91267667 - time (sec): 3335.19 - samples/sec: 186.14 - lr: 0.050000\n",
      "2023-04-02 20:26:01,026 ----------------------------------------------------------------------------------------------------\n",
      "2023-04-02 20:26:01,027 EPOCH 1 done: loss 0.9127 - lr 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1213/1213 [07:13<00:00,  2.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 20:33:15,155 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 20:33:15,420 DEV : loss 0.9240173697471619 - f1-score (micro avg)  0.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1213/1213 [07:14<00:00,  2.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 20:40:58,412 Evaluating as a multi-label problem: False\n",
      "2023-04-02 20:40:58,667 TEST : loss 0.923307478427887 - f1-score (micro avg)  0.5637\n",
      "2023-04-02 20:41:36,653 BAD EPOCHS (no improvement): 0\n",
      "2023-04-02 20:41:36,654 saving best model\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m ModelTrainer(classifier, corpus)\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m'\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m'\u001b[39;49m, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m, mini_batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, monitor_test\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# test_results, test_loss = trainer.evaluate(corpus.test)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[39m# print(f'Test loss: {test_loss:.4f}')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# print(test_results)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\flair\\trainers\\trainer.py:843\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self, base_path, learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, train_with_dev, train_with_test, monitor_train, monitor_test, main_evaluation_metric, scheduler, anneal_factor, patience, min_learning_rate, initial_extra_patience, optimizer, cycle_momentum, warmup_fraction, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, anneal_against_dev_loss, batch_growth_annealing, shuffle, param_selection_mode, write_weights, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, save_model_each_k_epochs, tensorboard_comment, use_swa, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, create_file_logs, create_loss_file, epoch, use_tensorboard, tensorboard_log_dir, metrics_for_tensorboard, optimizer_state_dict, scheduler_state_dict, save_optimizer_state, reduce_transformer_vocab, shuffle_first_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    837\u001b[0m     (\u001b[39mnot\u001b[39;00m train_with_dev \u001b[39mor\u001b[39;00m anneal_with_restarts \u001b[39mor\u001b[39;00m anneal_with_prestarts)\n\u001b[0;32m    838\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m param_selection_mode\n\u001b[0;32m    839\u001b[0m     \u001b[39mand\u001b[39;00m current_epoch_has_best_model_so_far\n\u001b[0;32m    840\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m use_final_model_for_eval\n\u001b[0;32m    841\u001b[0m ):\n\u001b[0;32m    842\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaving best model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 843\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave(base_path \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mbest-model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, checkpoint\u001b[39m=\u001b[39;49msave_optimizer_state)\n\u001b[0;32m    845\u001b[0m     \u001b[39mif\u001b[39;00m anneal_with_prestarts:\n\u001b[0;32m    846\u001b[0m         current_state_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:104\u001b[0m, in \u001b[0;36mModel.save\u001b[1;34m(self, model_file, checkpoint)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, model_file: Union[\u001b[39mstr\u001b[39m, Path], checkpoint: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    Saves the current model to the provided file.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    :param model_file: the model file\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     model_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_state_dict()\n\u001b[0;32m    106\u001b[0m     \u001b[39m# in Flair <0.9.1, optimizer and scheduler used to train model are not saved\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     optimizer \u001b[39m=\u001b[39m scheduler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\flair\\models\\text_classification_model.py:63\u001b[0m, in \u001b[0;36mTextClassifier._get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_state_dict\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     61\u001b[0m     model_state \u001b[39m=\u001b[39m {\n\u001b[0;32m     62\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_get_state_dict(),\n\u001b[1;32m---> 63\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdocument_embeddings\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings\u001b[39m.\u001b[39;49msave_embeddings(use_state_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m     64\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlabel_dictionary\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dictionary,\n\u001b[0;32m     65\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlabel_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_type,\n\u001b[0;32m     66\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmulti_label\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_label,\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmulti_label_threshold\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_label_threshold,\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mweight_dict\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_dict,\n\u001b[0;32m     69\u001b[0m     }\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m model_state\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\base.py:103\u001b[0m, in \u001b[0;36mEmbeddings.save_embeddings\u001b[1;34m(self, use_state_dict)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_embeddings\u001b[39m(\u001b[39mself\u001b[39m, use_state_dict: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 103\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_params()\n\u001b[0;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m use_state_dict:\n\u001b[0;32m    105\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\base.py:91\u001b[0m, in \u001b[0;36mEmbeddings.to_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_params\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('./', max_epochs=5, learning_rate=0.05, mini_batch_size=64, monitor_test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
